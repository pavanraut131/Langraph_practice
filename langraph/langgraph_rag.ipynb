{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash', google_api_key=os.getenv('GEMINI_KEY'))\n",
    "embeddings= GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=os.getenv('GEMINI_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain .text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('./agents.pdf')\n",
    "text = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "final_docs = text_splitter.split_documents(text)\n",
    "\n",
    "vector_db = Chroma.from_documents(final_docs, embeddings )\n",
    "retriever=  vector_db.as_retriever(search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'creationdate': '2025-02-14T12:31:53+05:00', 'creator': 'Adobe InDesign 19.1 (Windows)', 'moddate': '2025-02-14T12:33:59+05:00', 'page': 9, 'page_label': '10', 'producer': 'Adobe PDF Library 17.0', 'source': './agents.pdf', 'total_pages': 93, 'trapped': '/False'}\n",
      "provides a structured approach to defining and managing agents. Itâ€™s very straightforward\n",
      "page_content='agents along with their characteristics, examples, and when you can use them.' metadata={'creationdate': '2025-02-14T12:31:53+05:00', 'creator': 'Adobe InDesign 19.1 (Windows)', 'moddate': '2025-02-14T12:33:59+05:00', 'page': 9, 'page_label': '10', 'producer': 'Adobe PDF Library 17.0', 'source': './agents.pdf', 'total_pages': 93, 'trapped': '/False'}\n",
      "page_content='provides a structured approach to defining and managing agents. Itâ€™s very straightforward' metadata={'creationdate': '2025-02-14T12:31:53+05:00', 'creator': 'Adobe InDesign 19.1 (Windows)', 'moddate': '2025-02-14T12:33:59+05:00', 'page': 30, 'page_label': '31', 'producer': 'Adobe PDF Library 17.0', 'source': './agents.pdf', 'total_pages': 93, 'trapped': '/False'}\n",
      "page_content='Now that weâ€™ve learned what agents are and when to and when not to use them, itâ€™s time' metadata={'creationdate': '2025-02-14T12:31:53+05:00', 'creator': 'Adobe InDesign 19.1 (Windows)', 'moddate': '2025-02-14T12:33:59+05:00', 'page': 24, 'page_label': '25', 'producer': 'Adobe PDF Library 17.0', 'source': './agents.pdf', 'total_pages': 93, 'trapped': '/False'}\n"
     ]
    }
   ],
   "source": [
    "query = \"what is agents  \"\n",
    "docs = retriever.invoke(query)\n",
    "print(docs[0].metadata)\n",
    "print(docs[1].page_content)\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(AgentState):\n",
    "    message = AgentState[\"messages\"]\n",
    "    question = message[-1]\n",
    "\n",
    "    complete_prompt = \"Your Tasks is provide only the brief answer based on the user query.\\\n",
    "        Dont include too much reasoning. Following the user query {question}\"\n",
    "\n",
    "    response= llm.invoke(complete_prompt)\n",
    "    AgentState['messages'].append(response.content)\n",
    "    print(AgentState)\n",
    "    return AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2(AgentState):\n",
    "    messgae= AgentState['messages']\n",
    "    question = messgae[0]\n",
    "    template = \"\"\"Answer the following question based on the context\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    retreival_chain=(\n",
    "        {'context':retriever, 'question':RunnablePassthrough()}\n",
    "        |prompt\n",
    "        |llm\n",
    "        |StrOutputParser()\n",
    "    )\n",
    "    result = retreival_chain.invoke(question)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "workflow = Graph()\n",
    "workflow.add_node('llm', function1)\n",
    "workflow.add_node('rag', function2)\n",
    "workflow.add_edge('llm', 'rag')\n",
    "workflow.set_entry_point('llm')\n",
    "workflow.set_finish_point('rag')\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'messages':['tell me about agents ']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': ['tell me about agents ', 'Okay, I understand. I will provide brief answers, avoiding excessive reasoning.']}\n",
      "here is the output from llm\n",
      "-------\n",
      "{'messages': ['tell me about agents ', 'Okay, I understand. I will provide brief answers, avoiding excessive reasoning.']}\n",
      "\n",
      "\n",
      "here is the output from rag\n",
      "-------\n",
      "Agents have characteristics and examples, and there are specific times when you can and cannot use them.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app.stream(inputs):\n",
    "    for key,value in output.items():\n",
    "        print(f'here is the output from {key}')\n",
    "        print('-------')\n",
    "        print(value)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('indian_economy.txt')\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=190, chunk_overlap=50)\n",
    "final_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "db2 = Chroma.from_documents(final_docs, embeddings)\n",
    "retreiver2 =db2.as_retriever(search_kwargs={'k':3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Economic Reforms:\n",
      "1991 Liberalization: Opened the economy to global markets, reduced state control, and encouraged foreign investment.\n"
     ]
    }
   ],
   "source": [
    "query= \"tell me about Economic Reforms \"\n",
    "docs  = retreiver2.invoke(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages :Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class TopicSelection(BaseModel):\n",
    "    topic:str=Field(description='Selected Topic')\n",
    "    Reasoning:str=Field(description='Reasoning behind the toipic seelction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=TopicSelection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(state):\n",
    "    message = state['messages']\n",
    "    question = message[-1]\n",
    "    print(question)\n",
    "\n",
    "   \n",
    "    template=\"\"\"\n",
    "    Your task is to classify the given user query into one of the following categories: [India, Sports, Not Related]. \n",
    "    Only respond with the category name and nothing else.\n",
    "    \n",
    "\n",
    "    User query: {question}\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template,\n",
    "                            input_variables=[question],\n",
    "                            partial_variables={\n",
    "                                'format_instructions':parser.get_format_instructions()\n",
    "                            })\n",
    "    chain = prompt|llm|parser\n",
    "    resposne = chain.invoke({'question':question, 'format_instructions':parser.get_format_instructions()})\n",
    "    print(resposne)\n",
    "    return {'messages':[resposne.topic]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell me about indian economy\n",
      "topic='India' Reasoning=\"The query explicitly mentions 'indian economy', which directly relates to India.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': ['India']}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = {\"messages\":[\"tell me about indian economy\"]}\n",
    "function1(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2(state):\n",
    "    messages = state['messages']\n",
    "    question = messages[0]\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\"\"\"\n",
    "    prompt= ChatPromptTemplate.from_template(template)\n",
    "    retrevial_chain = (\n",
    "        RunnableMap({'context':retreiver2, \"question\":RunnablePassthrough() })\n",
    "        |prompt\n",
    "        |llm\n",
    "        |StrOutputParser()\n",
    "    )\n",
    "    result = retrevial_chain.invoke(question)\n",
    "    return {'messages':[result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(state):\n",
    "    print('-> Calling LLM ->')\n",
    "\n",
    "    messages = state['messages']\n",
    "    print(messages)\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    # Normal LLM call\n",
    "    complete_query = \"Answer the follow question with you knowledge of the real world. Following is the user question: \" + question\n",
    "    response = llm.invoke(complete_query)\n",
    "    return {\"messages\": [response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Calling LLM ->\n",
      "['tell me about agents in langraph']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': ['Okay, let\\'s talk about Agents in the context of LangGraph.\\n\\nIt\\'s important to clarify that LangGraph itself is a *framework for building stateful, multi-actor applications*. It\\'s *not* a pre-built agent implementation like you might find in LangChain or AutoGen. LangGraph provides the *infrastructure* to build sophisticated agent systems, but you still need to define the specific agents and their behaviors.\\n\\nHere\\'s a breakdown of how agents fit into the LangGraph world:\\n\\n**1. What is an Agent (in this context)?**\\n\\nIn the context of LangGraph, an \"agent\" is essentially a *node* within your graph that represents an autonomous entity. This entity:\\n\\n*   **Receives Input (State):**  It takes in a piece of the graph\\'s current state. This state could contain:\\n    *   User queries.\\n    *   Results from previous agent actions.\\n    *   Contextual information (e.g., current date/time, data from a database).\\n*   **Processes the Input:** It uses some logic (often involving a Language Model, but not necessarily) to decide what action to take.\\n*   **Takes an Action:** Based on its processing, the agent decides on an action.  This action could be:\\n    *   Calling a tool (e.g., a search engine, a calculator, a database query tool).\\n    *   Sending a message to another agent.\\n    *   Updating the graph\\'s state.\\n    *   Providing a final answer to the user.\\n*   **Returns an Output:**  The output of the agent (the result of its action) is then used to update the graph\\'s state and potentially trigger other agents.\\n\\n**2. How LangGraph Helps Build Agents:**\\n\\nLangGraph simplifies building agent systems by providing:\\n\\n*   **State Management:**  LangGraph handles the complex task of managing the state of your application.  Each node (agent) in the graph can access and update the shared state. This is crucial for maintaining context across multiple agent interactions.\\n*   **Graph Structure:**  You define the connections between agents (the \"edges\" in the graph). This determines the flow of information and control within your system. You can create loops, conditional branches, and other complex interaction patterns.\\n*   **Concurrency and Parallelism:** LangGraph can handle multiple agents running concurrently, which can significantly speed up the overall process.\\n*   **Observability:** LangGraph offers tools for monitoring and debugging your agent systems. You can track the state of the graph, the actions taken by each agent, and the overall flow of execution.\\n*   **Flexibility:** You have complete control over the internal logic of each agent. You can use LangChain chains, custom code, or any other technique that suits your needs.\\n\\n**3. Key Components within LangGraph for Building Agents:**\\n\\n*   **Nodes:** As mentioned, agents are implemented as nodes in the LangGraph. Each node represents a step in the process and is responsible for performing a specific task.\\n*   **Edges:** Edges define the connections between nodes, specifying the flow of information and control.  You can have conditional edges that determine the next node to execute based on the current state.\\n*   **State:** The state represents the shared context of the application. Agents can read and update the state, allowing them to coordinate their actions and maintain a consistent view of the world.\\n*   **Graph:** The graph is the overall structure that connects the nodes and edges, defining the flow of execution.\\n\\n**4.  Example Scenarios:**\\n\\nHere are some examples of how you might use LangGraph to build agent systems:\\n\\n*   **Customer Service Chatbot:**\\n    *   Agent 1 (Initial Intake):  Greets the user and asks about their issue.\\n    *   Agent 2 (Knowledge Base Search):  Searches a knowledge base for relevant information.\\n    *   Agent 3 (Troubleshooting):  Asks the user a series of questions to diagnose the problem.\\n    *   Agent 4 (Escalation): If the problem cannot be resolved, escalate to a human agent.\\n*   **Research Assistant:**\\n    *   Agent 1 (Query Formulation):  Refines the user\\'s query.\\n    *   Agent 2 (Web Search):  Searches the web for relevant information.\\n    *   Agent 3 (Document Summarization): Summarizes the search results.\\n    *   Agent 4 (Report Generation):  Writes a report based on the summarized information.\\n*   **Code Generation:**\\n    *   Agent 1 (Requirement Gathering): Asks the user for a detailed description of the desired software.\\n    *   Agent 2 (Code Design): Designs the overall architecture of the software.\\n    *   Agent 3 (Code Implementation): Writes the code for the different modules.\\n    *   Agent 4 (Testing): Tests the code to ensure it meets the requirements.\\n\\n**5.  Difference from LangChain Agents:**\\n\\nLangChain agents are often pre-built solutions with a specific structure (agent + tools + loop). LangGraph gives you *more fine-grained control* over the entire process. You define the architecture, the state, and the interactions between agents. This allows you to create much more complex and customized agent systems.  Think of LangChain agents as a ready-to-go car, while LangGraph is like providing the parts and tools to build your own vehicle.\\n\\n**In Summary:**\\n\\nLangGraph is a powerful framework for building complex, stateful agent systems. It provides the infrastructure for managing state, defining agent interactions, and coordinating their actions.  While it doesn\\'t provide pre-built agents, it gives you the flexibility and control to create highly customized solutions for a wide range of applications. You essentially define the \"agents\" as nodes in your LangGraph and connect them using edges to create a workflow.']}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state={\"messages\":[\"tell me about agents in langraph\"]}\n",
    "function_3(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state):\n",
    "    print(\"---router state\")\n",
    "\n",
    "    messages = state['messages']\n",
    "    last_messages =messages[-1]\n",
    "    print(last_messages)\n",
    "    if 'India' in last_messages:\n",
    "        return \"RAG_CALL\"\n",
    "    else:\n",
    "        return \"LLM_CALL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "workflow5 = StateGraph(AgentState)\n",
    "workflow5.add_node('agent', function1)\n",
    "workflow5.add_node('Rag', function2)\n",
    "workflow5.add_node('llm', function_3)\n",
    "\n",
    "workflow5.set_entry_point('agent')\n",
    "workflow5.add_conditional_edges(\n",
    "    'agent',\n",
    "    router,\n",
    "    {\n",
    "        \"RAG_CALL\":\"Rag\",\n",
    "        \"LLM_CALL\":'llm'\n",
    "    }\n",
    ")\n",
    "workflow5.add_edge('Rag', END)\n",
    "workflow5.add_edge('llm', END)\n",
    "\n",
    "app = workflow5.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is the president of USa  \n",
      "topic='Not Related' Reasoning='The query is about the president of the USA, which is not related to India or Sports.'\n",
      "---router state\n",
      "Not Related\n",
      "-> Calling LLM ->\n",
      "['who is the president of USa  ', 'Not Related']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': ['who is the president of USa  ',\n",
       "  'Not Related',\n",
       "  'The President of the United States is Joe Biden.']}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = app.invoke({'messages':['who is the president of USa  ']})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
